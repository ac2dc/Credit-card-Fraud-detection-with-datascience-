{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Introduction \n","#### It is a end to end project for credit card fraud detection using datascience"]},{"cell_type":"markdown","metadata":{},"source":["### Various Step Implimented :\n","1. Importing the required packages.\n","2. Importing data.\n","3. Data preprocessing.\n","4. Feature selection and data spilting.\n","5. Building models.\n","6. Evaluating the model."]},{"cell_type":"markdown","metadata":{},"source":["## Importing the packages :\n","This project required various packages such as Pandas for working with data, Numpy to work with arrays, sci-kit learn for data split and building models "]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-12-22T08:44:49.664289Z","iopub.status.busy":"2021-12-22T08:44:49.663614Z","iopub.status.idle":"2021-12-22T08:44:50.953721Z","shell.execute_reply":"2021-12-22T08:44:50.952923Z","shell.execute_reply.started":"2021-12-22T08:44:49.664151Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","import itertools\n","from termcolor import colored as cl\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC \n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier \n","\n","from sklearn.metrics import confusion_matrix \n","from sklearn.metrics import accuracy_score \n","from sklearn.metrics import f1_score \n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"markdown","metadata":{},"source":["## Importing Data\n","Let's import the data for analysis, This dataset is taken from [kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud) . It contain V28 features along with other features, I am gonna drop the time feature which have no importance right now."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T08:44:50.955263Z","iopub.status.busy":"2021-12-22T08:44:50.955055Z","iopub.status.idle":"2021-12-22T08:44:54.693085Z","shell.execute_reply":"2021-12-22T08:44:54.692098Z","shell.execute_reply.started":"2021-12-22T08:44:50.955239Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('data/creditcard.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T08:44:54.694906Z","iopub.status.busy":"2021-12-22T08:44:54.694530Z","iopub.status.idle":"2021-12-22T08:44:54.769266Z","shell.execute_reply":"2021-12-22T08:44:54.768226Z","shell.execute_reply.started":"2021-12-22T08:44:54.694851Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>V10</th>\n","      <th>...</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>Amount</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-1.359807</td>\n","      <td>-0.072781</td>\n","      <td>2.536347</td>\n","      <td>1.378155</td>\n","      <td>-0.338321</td>\n","      <td>0.462388</td>\n","      <td>0.239599</td>\n","      <td>0.098698</td>\n","      <td>0.363787</td>\n","      <td>0.090794</td>\n","      <td>...</td>\n","      <td>-0.018307</td>\n","      <td>0.277838</td>\n","      <td>-0.110474</td>\n","      <td>0.066928</td>\n","      <td>0.128539</td>\n","      <td>-0.189115</td>\n","      <td>0.133558</td>\n","      <td>-0.021053</td>\n","      <td>149.62</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.191857</td>\n","      <td>0.266151</td>\n","      <td>0.166480</td>\n","      <td>0.448154</td>\n","      <td>0.060018</td>\n","      <td>-0.082361</td>\n","      <td>-0.078803</td>\n","      <td>0.085102</td>\n","      <td>-0.255425</td>\n","      <td>-0.166974</td>\n","      <td>...</td>\n","      <td>-0.225775</td>\n","      <td>-0.638672</td>\n","      <td>0.101288</td>\n","      <td>-0.339846</td>\n","      <td>0.167170</td>\n","      <td>0.125895</td>\n","      <td>-0.008983</td>\n","      <td>0.014724</td>\n","      <td>2.69</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-1.358354</td>\n","      <td>-1.340163</td>\n","      <td>1.773209</td>\n","      <td>0.379780</td>\n","      <td>-0.503198</td>\n","      <td>1.800499</td>\n","      <td>0.791461</td>\n","      <td>0.247676</td>\n","      <td>-1.514654</td>\n","      <td>0.207643</td>\n","      <td>...</td>\n","      <td>0.247998</td>\n","      <td>0.771679</td>\n","      <td>0.909412</td>\n","      <td>-0.689281</td>\n","      <td>-0.327642</td>\n","      <td>-0.139097</td>\n","      <td>-0.055353</td>\n","      <td>-0.059752</td>\n","      <td>378.66</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.966272</td>\n","      <td>-0.185226</td>\n","      <td>1.792993</td>\n","      <td>-0.863291</td>\n","      <td>-0.010309</td>\n","      <td>1.247203</td>\n","      <td>0.237609</td>\n","      <td>0.377436</td>\n","      <td>-1.387024</td>\n","      <td>-0.054952</td>\n","      <td>...</td>\n","      <td>-0.108300</td>\n","      <td>0.005274</td>\n","      <td>-0.190321</td>\n","      <td>-1.175575</td>\n","      <td>0.647376</td>\n","      <td>-0.221929</td>\n","      <td>0.062723</td>\n","      <td>0.061458</td>\n","      <td>123.50</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-1.158233</td>\n","      <td>0.877737</td>\n","      <td>1.548718</td>\n","      <td>0.403034</td>\n","      <td>-0.407193</td>\n","      <td>0.095921</td>\n","      <td>0.592941</td>\n","      <td>-0.270533</td>\n","      <td>0.817739</td>\n","      <td>0.753074</td>\n","      <td>...</td>\n","      <td>-0.009431</td>\n","      <td>0.798278</td>\n","      <td>-0.137458</td>\n","      <td>0.141267</td>\n","      <td>-0.206010</td>\n","      <td>0.502292</td>\n","      <td>0.219422</td>\n","      <td>0.215153</td>\n","      <td>69.99</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 30 columns</p>\n","</div>"],"text/plain":["         V1        V2        V3        V4        V5        V6        V7  \\\n","0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n","1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n","2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n","3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n","4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n","\n","         V8        V9       V10  ...       V21       V22       V23       V24  \\\n","0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n","1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n","2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n","3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n","4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n","\n","        V25       V26       V27       V28  Amount  Class  \n","0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n","1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n","2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n","3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n","4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n","\n","[5 rows x 30 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.drop('Time', axis=1, inplace = True)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Data Processing and EDA :\n","Now let's have a look at the ammount of fraud and non-fraud cases are there in our dataset.Along with that, let's also compute the percentage of fraud cases in the overall recorded transaction."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T08:44:54.771659Z","iopub.status.busy":"2021-12-22T08:44:54.770972Z","iopub.status.idle":"2021-12-22T08:44:54.821752Z","shell.execute_reply":"2021-12-22T08:44:54.820880Z","shell.execute_reply.started":"2021-12-22T08:44:54.771622Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[5m\u001b[1mCASE COUNT\u001b[0m\n","\u001b[1m--------------------------------------------\u001b[0m\n","\u001b[1mTotal number of cases are 284807\u001b[0m\n","\u001b[1mNumber of Non-fraud cases are 284315\u001b[0m\n","\u001b[1mNumber of Non-fraud cases are 492\u001b[0m\n","\u001b[1mPercentage of fraud cases is 0.17\u001b[0m\n","\u001b[1m--------------------------------------------\u001b[0m\n"]}],"source":["cases = len(df)\n","nonfraud_count = len(df[df.Class == 0])\n","fraud_count = len(df[df.Class == 1])\n","fraud_percentage = round(fraud_count/nonfraud_count*100, 2)\n","\n","print(cl('CASE COUNT', attrs = ['bold', 'blink']))\n","print(cl('--------------------------------------------', attrs = ['bold']))\n","print(cl('Total number of cases are {}'.format(cases), attrs = ['bold']))\n","print(cl('Number of Non-fraud cases are {}'.format(nonfraud_count), attrs = ['bold']))\n","print(cl('Number of Non-fraud cases are {}'.format(fraud_count), attrs = ['bold']))\n","print(cl('Percentage of fraud cases is {}'.format(fraud_percentage), attrs = ['bold']))\n","print(cl('--------------------------------------------', attrs = ['bold']))"]},{"cell_type":"markdown","metadata":{},"source":["**As I can see, there are 284,807 sample are in total.Out of which  there are only 492 fraud cases, Which sums up to only 0.17 percent of total.This is a example of imbalance data and needs to be handled carefully, otherwise it'll overfit.**"]},{"cell_type":"markdown","metadata":{},"source":["## Statistical view of data"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T08:45:28.727969Z","iopub.status.busy":"2021-12-22T08:45:28.727008Z","iopub.status.idle":"2021-12-22T08:45:28.785112Z","shell.execute_reply":"2021-12-22T08:45:28.784244Z","shell.execute_reply.started":"2021-12-22T08:45:28.727919Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1mCASE AMOUNT STATISTICS\u001b[0m\n","\u001b[1m----------------------------\u001b[0m\n","\u001b[1mNON-FRAUD CASE AMOUNT STATS\u001b[0m\n","count    284315.000000\n","mean         88.291022\n","std         250.105092\n","min           0.000000\n","25%           5.650000\n","50%          22.000000\n","75%          77.050000\n","max       25691.160000\n","Name: Amount, dtype: float64\n","\u001b[1m----------------------------\u001b[0m\n","\u001b[1mFRAUD CASE AMOUNT STATS\u001b[0m\n","count     492.000000\n","mean      122.211321\n","std       256.683288\n","min         0.000000\n","25%         1.000000\n","50%         9.250000\n","75%       105.890000\n","max      2125.870000\n","Name: Amount, dtype: float64\n","\u001b[1m----------------------------\u001b[0m\n"]}],"source":["nonfraud_cases = df[df.Class == 0]\n","fraud_cases = df[df.Class == 1]\n","\n","print(cl('CASE AMOUNT STATISTICS', attrs = ['bold']))\n","print(cl('----------------------------', attrs = ['bold']))\n","print(cl('NON-FRAUD CASE AMOUNT STATS', attrs = ['bold']))\n","print(nonfraud_cases.Amount.describe())\n","print(cl('----------------------------', attrs = ['bold']))\n","print(cl('FRAUD CASE AMOUNT STATS', attrs = ['bold']))\n","print(fraud_cases.Amount.describe())\n","print(cl('----------------------------', attrs = ['bold']))"]},{"cell_type":"markdown","metadata":{},"source":["## Normalisation using Standerdscaler\n","values in the ‘Amount’ variable are varying enormously when compared to the rest of the variables. To reduce its wide range of values, we can normalize it using the ‘StandardScaler’ method in python.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T08:49:47.086079Z","iopub.status.busy":"2021-12-22T08:49:47.085723Z","iopub.status.idle":"2021-12-22T08:49:47.100162Z","shell.execute_reply":"2021-12-22T08:49:47.099187Z","shell.execute_reply.started":"2021-12-22T08:49:47.086040Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m0    0.244964\n","1   -0.342475\n","2    1.160686\n","3    0.140534\n","4   -0.073403\n","5   -0.338556\n","6   -0.333279\n","7   -0.190107\n","8    0.019392\n","9   -0.338516\n","Name: Amount, dtype: float64\u001b[0m\n"]}],"source":["sc = StandardScaler()\n","amount = df['Amount'].values\n","\n","df[\"Amount\"] = sc.fit_transform(amount.reshape(-1, 1))\n","\n","print(cl(df['Amount'].head(10), attrs=['bold']))"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Selection and Data split\n","This is the very crucial step of the process,We are going to define independent (X) and the depended variable (y), I will split the data into training and testing set which is furthur used for modelling and evaluating."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1mX_train samples : \u001b[0m [[-1.11504743  1.03558276  0.80071244 -1.06039825  0.03262117  0.85342216\n","  -0.61424348 -3.23116112  1.53994798 -0.81690879 -1.30559201  0.1081772\n","  -0.85960958 -0.07193421  0.90665563 -1.72092961  0.79785322 -0.0067594\n","   1.95677806 -0.64489556  3.02038533 -0.53961798  0.03315649 -0.77494577\n","   0.10586781 -0.43085348  0.22973694 -0.0705913  -0.30145418]]\n","\u001b[1mX_test samples : \u001b[0m [[-0.32333357  1.05745525 -0.04834115 -0.60720431  1.25982115 -0.09176072\n","   1.1591015  -0.12433461 -0.17463954 -1.64440065 -1.11886302  0.20264731\n","   1.14596495 -1.80235956 -0.24717793 -0.06094535  0.84660574  0.37945439\n","   0.84726224  0.18640942 -0.20709827 -0.43389027 -0.26161328 -0.04665061\n","   0.2115123   0.00829721  0.10849443  0.16113917 -0.19330595]]\n","\u001b[1my_train samples : \u001b[0m [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","\u001b[1my_test samples : \u001b[0m [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"]}],"source":["X = df.drop('Class', axis = 1).values\n","y = df['Class'].values\n","\n","X_train,X_test,y_train,y_test = train_test_split(X, y, test_size= 0.2, random_state=0)\n","\n","print(cl('X_train samples : ', attrs = ['bold']), X_train[:1])\n","print(cl('X_test samples : ', attrs = ['bold']), X_test[0:1])\n","print(cl('y_train samples : ', attrs = ['bold']), y_train[0:20])\n","print(cl('y_test samples : ', attrs = ['bold']), y_test[0:20])"]},{"cell_type":"markdown","metadata":{},"source":["I have uses only 20 percent data as test data to validate later."]},{"cell_type":"markdown","metadata":{},"source":["### Modelling :\n","I am going to build six diffrent types of classification models namely Decision Tree, K-Nearest Neighbor(KNN), Logistic Regression, Support vector machine, Random forest and XGBoost."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ac2dc/miniconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[04:13:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]}],"source":["# 1.Decision Tree\n","\n","tree_model = DecisionTreeClassifier(max_depth= 4, criterion= 'entropy')\n","tree_model.fit(X_train, y_train)\n","tree_yhat = tree_model.predict(X_test)\n","\n","# 2. KNN\n","\n","n = 5\n","knn = KNeighborsClassifier(n_neighbors= n)\n","knn.fit(X_train, y_train)\n","knn_yhat = knn.predict(X_test)\n","\n","# 3. Logistic regression\n","\n","lr = LogisticRegression()\n","lr.fit(X_train, y_train)\n","lr_yhat = lr.predict(X_test)\n","\n","# 4. SVM \n","\n","svm = SVC()\n","svm.fit(X_train, y_train)\n","svm_yhat = svm.predict(X_test)\n","\n","# 5. RandomForestClassifier\n","\n","rf = RandomForestClassifier()\n","rf.fit(X_train, y_train)\n","rf_yhat = rf.predict(X_test)\n","\n","# 6. XGBoost\n","\n","xgb = XGBClassifier(max_depth = 4)\n","xgb.fit(X_train, y_train)\n","xgb_yhat = xgb.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["Starting with the decision tree, we have used the ‘DecisionTreeClassifier’ algorithm to build the model. Inside the algorithm, we have mentioned the ‘max_depth’ to be ‘4’ which means we are allowing the tree to split four times and the ‘criterion’ to be ‘entropy’ which is most similar to the ‘max_depth’ but determines when to stop splitting the tree. Finally, we have fitted and stored the predicted values into the ‘tree_yhat’ variable.\n","\n","Next is the K-Nearest Neighbors (KNN). We have built the model using the ‘KNeighborsClassifier’ algorithm and mentioned the ‘n_neighbors’ to be ‘5’. The value of the ‘n_neighbors’ is randomly selected but can be chosen optimistically through iterating a range of values, followed by fitting and storing the predicted values into the ‘knn_yhat’ variable.\n","\n","There is nothing much to explain about the code for Logistic regression as we kept the model in a way more simplistic manner by using the ‘LogisticRegression’ algorithm and as usual, fitted and stored the predicted variables in the ‘lr_yhat’ variable.\n","\n","We built the Support Vector Machine model using the ‘SVC’ algorithm and we didn’t mention anything inside the algorithm as we managed to use the default kernel which is the ‘rbf’ kernel. After that, we stored the predicted values into the ‘svm_yhat’ after fitting the model.\n","\n","The next model is the Random forest model which we built using the ‘RandomForestClassifier’ algorithm and we mentioned the ‘max_depth’ to be 4 just like how we did to build the decision tree model. Finally, fitting and storing the values into the ‘rf_yhat’. Remember that the main difference between the decision tree and the random forest is that, decision tree uses the entire dataset to construct a single model whereas, the random forest uses randomly selected features to construct multiple models. That’s the reason why the random forest model is used versus a decision tree.\n","\n","Our final model is the XGBoost model. We built the model using the ‘XGBClassifier’ algorithm provided by the xgboost package. We mentioned the ‘max_depth’ to be 4 and finally, fitted and stored the predicted values into the ‘xgb_yhat’.\n","\n","With that, we have successfully built our six types of classification models and interpreted the code for easy understanding. Our next step is to evaluate each of the models and find which is the most suitable one for our case.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation\n","As our model are done , I;m going to evaluate our built models using the evaluation matrics provided by sci-kit learn library. Various evaluation matrics I am goin to use are the accurancy score metric, f1 score matrics, and finally the confusion matrix."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m\u001b[33mACCURACY SCORE\u001b[0m\n","\u001b[1m------------------------------------------------------------------------\u001b[0m\n","\u001b[1mAccuracy score of the Decision Tree model is 0.9993679997191109\u001b[0m\n","\u001b[1m------------------------------------------------------------------------\u001b[0m\n","\u001b[1m\u001b[32mAccuracy score of the KNN model is 0.9995259997893332\u001b[0m\n","\u001b[1m------------------------------------------------------------------------\u001b[0m\n","\u001b[1m\u001b[31mAccuracy score of the Logistic Regression model is 0.9991924440855307\u001b[0m\n","\u001b[1m------------------------------------------------------------------------\u001b[0m\n","\u001b[1mAccuracy score of the SVM model is 0.9993153330290369\u001b[0m\n","\u001b[1m------------------------------------------------------------------------\u001b[0m\n","\u001b[1mAccuracy score of the Random Forest Tree model is 0.9995084442259752\u001b[0m\n","\u001b[1m------------------------------------------------------------------------\u001b[0m\n","\u001b[1mAccuracy score of the XGBoost model is 0.9994908886626171\u001b[0m\n","\u001b[1m------------------------------------------------------------------------\u001b[0m\n"]}],"source":["# 1. Accuracy score\n","\n","print(cl('ACCURACY SCORE', attrs = ['bold'], color='yellow'))\n","print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n","print(cl('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test, tree_yhat)), attrs = ['bold']))\n","print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n","print(cl('Accuracy score of the KNN model is {}'.format(accuracy_score(y_test, knn_yhat)), attrs = ['bold'], color = 'green'))\n","print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n","print(cl('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test, lr_yhat)), attrs = ['bold'], color = 'red'))\n","print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n","print(cl('Accuracy score of the SVM model is {}'.format(accuracy_score(y_test, svm_yhat)), attrs = ['bold']))\n","print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n","print(cl('Accuracy score of the Random Forest Tree model is {}'.format(accuracy_score(y_test, rf_yhat)), attrs = ['bold']))\n","print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n","print(cl('Accuracy score of the XGBoost model is {}'.format(accuracy_score(y_test, xgb_yhat)), attrs = ['bold']))\n","print(cl('------------------------------------------------------------------------', attrs = ['bold']))"]},{"cell_type":"markdown","metadata":{},"source":["KNN model revels to be most accurate model of all , However, on rounding up every model , it shows 0.99 (99% accurate) which is very good score. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":4}
